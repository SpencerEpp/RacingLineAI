{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Pipeline for Racing Line Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "# Quick check to ensure model will run on GPU\n",
    "import torch\n",
    "import cerberus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.serialization\n",
    "torch.serialization.add_safe_globals([MinMaxScaler])\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "\n",
    "# config = {\n",
    "#     \"model_type\": \"lstm\", # Options: \"lstm\", \"cnn\", \"tcn\", \"transformer\" \n",
    "#     \"seed\": 42,\n",
    "#     \"input_size\": 6,\n",
    "#     \"output_size\": 3,\n",
    "#     \"train_split\": 0.8,\n",
    "#     \"num_epochs\": 50,\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"hidden_size\": 128,\n",
    "#     \"num_layers\": 2,\n",
    "#     \"dropout\": 0.2,\n",
    "#     \"seq_len\": 150,\n",
    "#     \"patience\": 10,\n",
    "#     \"pin_memory\": True,\n",
    "#     \"bidirectional\": False,\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"train_data_path\": \"./data/extracted_track_data/\",\n",
    "#     \"test_data_path\": \"./data/testing_layouts/\",\n",
    "#     \"model_save_path\": \"./models/testing_racing_line_lstm.pt\",\n",
    "#     \"input_cols\": [\"left_x\",\"left_y\",\"left_z\",\"right_x\",\"right_y\",\"right_z\"],\n",
    "#     \"output_cols\": [\"x\",\"y\",\"z\"]\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"input_size\": 6,\n",
    "    #model params\n",
    "    \"hidden1\":128,\n",
    "    \"kern_size1\":3,\n",
    "    \"kern_size2\":3,\n",
    "    \"padding1\": 2,\n",
    "    \"padding2\": 4,\n",
    "    \"dilation1\": 2,\n",
    "    \"dilation2\": 4,\n",
    "    \"pos_head_sz\": 64,\n",
    "    \"cont_head_sz\": 64,\n",
    "    #administrative stuff\n",
    "    \"device\":\"cuda\",\n",
    "    \"input_cols\": [\"left_x\",\"left_y\",\"left_z\",\"right_x\",\"right_y\",\"right_z\"],\n",
    "    \"output_pos_cols\": [\"x\",\"y\",\"z\"],\n",
    "    \"output_cont_cols\": [\"speed\", \"gas\", \"brake\", \"side_left\", \"side_right\"],\n",
    "    \"patience\": 10,\n",
    "    #\"train_data_path\": \"./data/extracted_track_data/\",\n",
    "    \"train_data_path\": \"./data/testing_layouts/\",\n",
    "    \"test_data_path\": \"./data/testing_layouts/\",\n",
    "    \"model_save_path\": \"./models/testing_racing_proto_cerberus.pt\",\n",
    "    \"pin_memory\": True,\n",
    "    #Training info,\n",
    "    \"num_epochs\": 100,\n",
    "    \"long_epochs\": 35,\n",
    "    \"medium_epochs\": 35,\n",
    "    \"train_split\": 0.8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"long_seq_len\": 600,\n",
    "    \"med_seq_len\": 300,\n",
    "    \"short_seq_len\": 150\n",
    "}\n",
    "\n",
    "random.seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed(config[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RacingLineDataset(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.inputs, self.pos_targets, self.cont_targets = [], [], []\n",
    "        self.scaler_x = MinMaxScaler()\n",
    "        self.scaler_y = MinMaxScaler()\n",
    "        self.scaler_z = MinMaxScaler()\n",
    "        all_X, all_Y, all_Z = [], [], []\n",
    "        train_files = sorted(glob(os.path.join(config[\"train_data_path\"], \"*.csv\")))\n",
    "        test_files = sorted(glob(os.path.join(config[\"test_data_path\"], \"*.csv\")))\n",
    "\n",
    "        # === First pass: Collect data for global fitting ===\n",
    "        for file in train_files:\n",
    "            df = pd.read_csv(file)\n",
    "            X = df[config[\"input_cols\"]].values\n",
    "            Y = df[config[\"output_pos_cols\"]].values\n",
    "            Z = df[config[\"output_cont_cols\"]].values\n",
    "            all_X.append(X)\n",
    "            all_Y.append(Y)\n",
    "            all_Z.append(Z)\n",
    "        for file in test_files:\n",
    "            df = pd.read_csv(file)\n",
    "            X = df[config[\"input_cols\"]].values\n",
    "            Y = df[config[\"output_pos_cols\"]].values\n",
    "            Z = df[config[\"output_cont_cols\"]].values\n",
    "            all_X.append(X)\n",
    "            all_Y.append(Y)\n",
    "            all_Z.append(Z)\n",
    "        all_X = np.vstack(all_X)\n",
    "        all_Y = np.vstack(all_Y)\n",
    "        all_Z = np.vstack(all_Z)\n",
    "        self.scaler_x.fit(all_X)\n",
    "        self.scaler_y.fit(all_Y)\n",
    "        self.scaler_z.fit(all_Z)\n",
    "\n",
    "        # === Second pass: Normalize and extract sequences ===\n",
    "        for file in train_files:\n",
    "            df = pd.read_csv(file)\n",
    "            is_circular = is_circular_track(df, config[\"output_cols\"])\n",
    "            X = self.scaler_x.transform(df[config[\"input_cols\"]].values)\n",
    "            Y = self.scaler_y.transform(df[config[\"output_pos_cols\"]].values)\n",
    "            Z = self.scaler_z.transform(df[config[\"output_cont_cols\"]].values)\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                self.inputs.append(get_centered_sequence(X, i, config[\"seq_len\"], is_circular))\n",
    "                self.pos_targets.append(Y[i])\n",
    "                self.cont_targets.append(Z[i])\n",
    "                \n",
    "        self.inputs = torch.tensor(np.array(self.inputs), dtype=torch.float32).to(config[\"device\"])\n",
    "        self.pos_targets = torch.tensor(np.array(self.pos_targets), dtype=torch.float32).to(config[\"device\"])\n",
    "        self.cont_targets = torch.tensor(np.array(self.cont_targets), dtype=torch.float32).to(config[\"device\"])\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx].permute(1,0), self.pos_targets[idx], self.cont_targets[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cerberus (torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        #Thanks Resnet!\n",
    "        class ResBlock(torch.nn.Module):\n",
    "            def __init__(self, in_chans, out_chans, kern_size, padding, dilation):\n",
    "                super().__init__()\n",
    "                self.conv1 = torch.nn.Conv1d(in_chans, out_chans, kern_size, padding=padding, dilation=dilation)\n",
    "                self.relu = torch.nn.ReLU()\n",
    "                self.conv2 = torch.nn.Conv1d(out_chans, out_chans, kern_size, padding=padding, dilation=dilation)\n",
    "    \n",
    "                self.shortcut = torch.nn.Identity()\n",
    "                if in_chans != out_chans:\n",
    "                    self.shortcut = nn.Conv1d(in_chans, out_chans, kernel_size=1)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                residual = self.shortcut(x)\n",
    "                return self.relu(self.conv2(self.relu(self.conv1(x))) + residual)\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            ResBlock(config[\"input_size\"], config[\"hidden1\"], config[\"kern_size1\"], config[\"padding1\"], config[\"dilation1\"]),\n",
    "            ResBlock(config[\"hidden1\"], config[\"hidden1\"], config[\"kern_size2\"], config[\"padding2\"], config[\"dilation2\"])\n",
    "        )\n",
    "\n",
    "        #x, y, z\n",
    "        self.position_head = t.nn.Sequential(\n",
    "            torch.nn.Linear(config[\"hidden1\"], config[\"pos_head_sz\"]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(config[\"pos_head_sz\"], 3)\n",
    "        )\n",
    "\n",
    "        #speed, gas, brake, side-left, side-right\n",
    "        self.control_head = t.nn.Sequential(\n",
    "            torch.nn.Linear(config[\"hidden1\"] + 3, config[\"cont_head_sz\"]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(config[\"cont_head_sz\"], 5)\n",
    "        )\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = torch.mean(encoded, dim=2)\n",
    "\n",
    "        position = self.position_head(encoded)\n",
    "        control_in = torch.cat([encoded, position], dim=1)\n",
    "        control = self.control_head(control_in)\n",
    "\n",
    "        \n",
    "        return position, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation/Loss Functions ===\n",
    "class PositionLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=1.0):\n",
    "        super(PositionLoss, self).__init__()\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        loss = self.mse_loss(predicted, target)\n",
    "        return loss\n",
    "\n",
    "#we can add penalties to encourage smoother and more realistic driving here\n",
    "class ControlLoss(t.nn.Module):\n",
    "    def __init__(self, weight=1.0):\n",
    "        super(ControlLoss, self).__init__()\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, predicted, target):\n",
    "        loss = self.mse_loss(predicted, target)\n",
    "        return loss\n",
    "\n",
    "def evaluate_model(model, dataloader, pos_crit, con_crit, config):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch, Z_batch in dataloader:\n",
    "            X_batch, Y_batch, Z_batch = X_batch.to(config[\"device\"]), Y_batch.to(config[\"device\"]), Z_batch.to(config[\"device\"])\n",
    "            pos, cont = model(X_batch)\n",
    "            pos_loss = pos_crit(pos, Y_batch)\n",
    "            con_loss = con_crit(cont, Z_batch)\n",
    "            \n",
    "            loss = pos_loss + 0.5 * con_loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save and Load Model (with architecture) ===\n",
    "def save_model(model, config):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config,\n",
    "        # \"scaler_x\": scaler_x,\n",
    "        # \"scaler_y\": scaler_y,\n",
    "        # \"scaler_z\": scaler_z,\n",
    "    }, config[\"model_save_path\"])\n",
    "\n",
    "# def load_model(path):\n",
    "#     checkpoint = torch.load(path, map_location=config[\"device\"], weights_only=False)\n",
    "#     cfg = checkpoint[\"config\"]\n",
    "#     model = RacingLineLSTMWithAttention(cfg[\"input_size\"], cfg[\"hidden_size\"],\n",
    "#                                         cfg[\"output_size\"], cfg[\"num_layers\"],\n",
    "#                                         cfg[\"dropout\"], cfg[\"bidirectional\"])\n",
    "#     model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "#     model.to(cfg[\"device\"])\n",
    "#     return model, checkpoint[\"scaler_x\"], checkpoint[\"scaler_y\"]\n",
    "\n",
    "def load_model(path):\n",
    "    checkpoint = torch.load(path, map_location=config[\"device\"], weights_only=False)\n",
    "    cfg = checkpoint[\"config\"]\n",
    "    #model = get_model(cfg)\n",
    "    model = Cerberus(cfg)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(cfg[\"device\"])\n",
    "    # scaler_x = checkpoint[\"scaler_x\"]\n",
    "    # scaler_y = checkpoint[\"scaler_y\"]\n",
    "    # scaler_z = checkpoint[\"scaler_z\"]\n",
    "    return model #, scaler_x, scaler_y, scaler_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Function (with tqdm and validation) ===\n",
    "def train_model(model, long_dataset, medium_dataset, short_dataset, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    # = hybrid_loss #nn.MSELoss()\n",
    "\n",
    "    pos_crit = PositionLoss()\n",
    "    con_crit = ControlLoss(weight=0.5)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_patience = config[\"patience\"]\n",
    "    epochs_without_improvement = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    obar = tqdm(range(config[\"num_epochs\"]), desc=\"Epochs\")\n",
    "    for epoch in obar:\n",
    "        match epoch:\n",
    "            case 0:\n",
    "                train_loader = long_dataset[0]\n",
    "                val_loader = long_dataset[1]\n",
    "            case val if val == config[\"long_epochs\"]:\n",
    "                train_loader = medium_dataset[0]\n",
    "                val_loader = medium_dataset[1]\n",
    "            case val if val == config[\"medium_epochs\"]:\n",
    "                train_loader = medium_dataset[0]\n",
    "                val_loader = medium_dataset[1]\n",
    "            case _:\n",
    "                pass\n",
    "                \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\", leave=False)\n",
    "        for X_batch, Y_batch, Z_batch in pbar:\n",
    "            X_batch, Y_batch, Z_batch = X_batch.to(config[\"device\"]), Y_batch.to(config[\"device\"]), Z_batch.to(config[\"device\"])\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            p_pos, p_cont = model(X_batch)\n",
    "            pos_loss, con_loss = pos_crit(p_pos, Y_batch), con_crit(p_cont, Z_batch)\n",
    "            loss = pos_loss + 0.5 * con_loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": running_loss / (pbar.n + 1)})\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader, pos_crit, con_crit, config)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, config)\n",
    "            best_epoch = epoch+1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        obar.set_postfix({\"Train Loss\": running_loss/len(train_loader), \"Val Loss\": val_loss, \"Lr\": scheduler.get_last_lr()[0], \"best_epoch\": best_epoch})\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Full Pipeline ===\n",
    "def run_pipeline(train_data_path=config[\"train_data_path\"], test_data_path=config[\"test_data_path\"]):\n",
    "    print(\"Preparing dataset...\")\n",
    "    long_dataset = RacingLineDataset(\n",
    "        train_data_path,\n",
    "        test_data_path,\n",
    "        config[\"long_seq_len\"],\n",
    "    )\n",
    "    medium_dataset = RacingLineDataset(\n",
    "        train_data_path,\n",
    "        test_data_path,\n",
    "        config[\"med_seq_len\"],\n",
    "    )\n",
    "    short_dataset = RacingLineDataset(\n",
    "        train_data_path,\n",
    "        test_data_path,\n",
    "        config[\"short_seq_len\"],\n",
    "    )\n",
    "\n",
    "    datasets = [long_dataset, medium_dataset, short_dataset]\n",
    "    scaler_x, scaler_y, scaler_z = long_dataset.scaler_x, long_dataset.scaler_y, long_dataset.scaler_z\n",
    "    scalers = (scaler_x, scaler_y, scaler_z)\n",
    "    # === 80/20 train/val split with reproducibility ===\n",
    "    \n",
    "    loaders = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        train_len = int(len(dataset) * config[\"train_split\"])\n",
    "        val_len = len(dataset) - train_len\n",
    "        train_ds, val_ds = random_split(\n",
    "            dataset,\n",
    "            [train_len, val_len],\n",
    "            generator=torch.Generator().manual_seed(config[\"seed\"])\n",
    "        )\n",
    "        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=False, pin_memory=config[\"pin_memory\"])\n",
    "        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, pin_memory=config[\"pin_memory\"])\n",
    "        loaders.append([train_loader, val_loader])\n",
    "\n",
    "    # train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=False, pin_memory=config[\"pin_memory\"])\n",
    "    # val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, pin_memory=config[\"pin_memory\"])\n",
    "\n",
    "    print(f\"Initializing model: Cerberus\")\n",
    "    model = Cerberus(config=config).to(config[\"device\"])\n",
    "\n",
    "    print(\"Training started...\")\n",
    "    # train_losses, val_losses = train_model(model, train_loader, val_loader, config, scaler_x, scaler_y, scaler_z)\n",
    "    train_losses, val_losses = train_model(model, loaders[0], loaders[1], loaders[2], config, scalers)\n",
    "    print(f\"Training complete. Model saved to {config['model_save_path']}\")\n",
    "\n",
    "    # === Plot learning curve ===\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Initializing model: Cerberus\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd2265f48c446b0a352bb6adc5f3e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f771999e282549c2882abc7c52ce7b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Train Model ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(train_data_path, test_data_path)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining started...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# train_losses, val_losses = train_model(model, train_loader, val_loader, config, scaler_x, scaler_y, scaler_z)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining complete. Model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mmodel_save_path\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# === Plot learning curve ===\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, long_dataset, medium_dataset, short_dataset, config)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_loss < best_val_loss:\n\u001b[32m     57\u001b[39m     best_val_loss = val_loss\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     save_model(model, config, \u001b[43mscaler_x\u001b[49m, scaler_y, scaler_z)\n\u001b[32m     59\u001b[39m     best_epoch = epoch+\u001b[32m1\u001b[39m\n\u001b[32m     60\u001b[39m     epochs_without_improvement = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'scaler_x' is not defined"
     ]
    }
   ],
   "source": [
    "# === Train Model ===\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Inference on Testing Track Layouts (from coordinates only, doesnt take image for inference) ===\n",
    "def print_feature_accuracy(preds, trues, scaler_y, feature_names):\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "\n",
    "    print(f\"\\nPer-Feature Accuracy (%):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, name in enumerate(feature_names):\n",
    "        range_train = scaler_y.scale_[i]\n",
    "        range_test = trues[:, i].max() - trues[:, i].min()\n",
    "\n",
    "        if range_test == 0:\n",
    "            print(f\"{name:>16}: N/A (zero test range)\")\n",
    "            continue\n",
    "\n",
    "        mean_error = np.mean(np.abs(preds[:, i] - trues[:, i]))\n",
    "        acc_train = (1 - (mean_error / range_train)) * 100\n",
    "        acc_test = (1 - (mean_error / range_test)) * 100\n",
    "\n",
    "        acc_train = max(0.0, min(100.0, acc_train))\n",
    "        acc_test = max(0.0, min(100.0, acc_test))\n",
    "\n",
    "        print(f\"{name:>16}: {acc_test:6.2f}% (layout-based)   {acc_train:6.2f}% (train-scale)\")\n",
    "\n",
    "# === Circular Tracks ===\n",
    "def run_inference(data_folder, model_path):\n",
    "    print(\"Loading model and scalers...\")\n",
    "    #model, scaler_x, scaler_y, _ = load_model(model_path)\n",
    "    model = load_model(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Loading unseen layouts from:\", data_folder)\n",
    "    layout_files = sorted(glob(os.path.join(data_folder, \"*.csv\")))\n",
    "    total_layouts = len(layout_files)\n",
    "    print(f\"Found {total_layouts} layout files.\\n\")\n",
    "\n",
    "    for layout_index, layout_path in enumerate(layout_files):\n",
    "        layout_name = os.path.basename(layout_path)\n",
    "        print(f\"[{layout_index + 1}/{total_layouts}] Predicting layout: {layout_name}\")\n",
    "\n",
    "        df = pd.read_csv(layout_path)\n",
    "        X = df[config[\"input_cols\"]].values\n",
    "        Y = df[config[\"output_pos_cols\"]].values\n",
    "        #X_scaled = scaler_x.transform(X)\n",
    "        n = len(X_scaled)\n",
    "        preds_real = np.zeros_like(Y)\n",
    "        trues_real = Y.copy()\n",
    "\n",
    "        for i in tqdm(range(n), desc=f\"[{layout_index + 1}/{total_layouts}]\"):\n",
    "            #seq = np.array([X_scaled[(i + j) % n] for j in range(config[\"seq_len\"])])\n",
    "            seq = np.array([X[(i + j) % n] for j in range(config[\"seq_len\"])])\n",
    "            X_tensor = torch.tensor(seq.reshape(1, config[\"seq_len\"], -1), dtype=torch.float32).to(config[\"device\"])\n",
    "            X_tensor = X_tensor.permute(0, 2, 1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred, _ = model(X_tensor)\n",
    "                pred_scaled = pred.cpu().squeeze().numpy()\n",
    "                pred_real = scaler_y.inverse_transform(pred_scaled.reshape(1, -1))[0]\n",
    "                print(pred_real)\n",
    "\n",
    "            target_idx = (i + config[\"seq_len\"]) % n\n",
    "            preds_real[target_idx] += pred_real\n",
    "\n",
    "        # === Plot X/Z comparison ===\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(trues_real[:, 0], trues_real[:, 2], label=\"True\", linewidth=2)\n",
    "        plt.plot(preds_real[:, 0], preds_real[:, 2], label=\"Predicted\", linewidth=2, linestyle=\"--\")\n",
    "        plt.title(f\"X vs Z: {layout_name}\")\n",
    "        plt.xlabel(\"X Coordinate\")\n",
    "        plt.ylabel(\"Z Coordinate\")\n",
    "        plt.axis(\"equal\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # === Accuracy ===\n",
    "        print_feature_accuracy(preds_real, trues_real, scaler_y, config[\"output_pos_cols\"])\n",
    "\n",
    "        # === Spatial Error (X/Z only)\n",
    "        spatial_errors = np.linalg.norm(preds_real[:, [0, 2]] - trues_real[:, [0, 2]], axis=1)\n",
    "        mean_spatial_error = np.mean(spatial_errors)\n",
    "        max_spatial_error = np.max(spatial_errors)\n",
    "        print(f\"Mean X/Z spatial error: {mean_spatial_error:.2f}m, Max: {max_spatial_error:.2f}m\\n\")\n",
    "\n",
    "# === Non-Circular Tracks ===\n",
    "# def run_inference(data_folder, model_path):\n",
    "#     print(\"Loading model and scalers...\")\n",
    "#     model, scaler_x, scaler_y = load_model(model_path)\n",
    "#     model.eval()\n",
    "\n",
    "#     print(\"Loading unseen layouts from:\", data_folder)\n",
    "#     layout_files = sorted(glob(os.path.join(data_folder, \"*.csv\")))\n",
    "#     total_layouts = len(layout_files)\n",
    "#     print(f\"Found {total_layouts} layout files.\\n\")\n",
    "\n",
    "#     for layout_index, layout_path in enumerate(layout_files):\n",
    "#         layout_name = os.path.basename(layout_path)\n",
    "#         print(f\"[{layout_index + 1}/{total_layouts}] Predicting layout: {layout_name}\")\n",
    "\n",
    "#         # === Load layout data ===\n",
    "#         df = pd.read_csv(layout_path)\n",
    "#         X = df[config[\"input_cols\"]].values\n",
    "#         Y = df[config[\"output_cols\"]].values\n",
    "#         X_scaled = scaler_x.transform(X)\n",
    "#         n = len(X_scaled)\n",
    "#         seq_len = config[\"seq_len\"]\n",
    "\n",
    "#         preds_real = np.zeros_like(Y)\n",
    "#         trues_real = Y.copy()\n",
    "\n",
    "#         for i in tqdm(range(n), desc=f\"[{layout_index + 1}/{total_layouts}]\"):\n",
    "#             # Pad first `seq_len` frames with the first frame\n",
    "#             if i < seq_len:\n",
    "#                 pad = np.repeat(X_scaled[0:1], seq_len - i, axis=0)\n",
    "#                 seq = np.vstack([pad, X_scaled[0:i]])\n",
    "#             else:\n",
    "#                 seq = X_scaled[i - seq_len:i]\n",
    "\n",
    "#             X_tensor = torch.tensor(seq.reshape(1, seq_len, -1), dtype=torch.float32).to(config[\"device\"])\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 pred_scaled = model(X_tensor).cpu().squeeze().numpy()\n",
    "#                 pred_real = scaler_y.inverse_transform(pred_scaled.reshape(1, -1))[0]\n",
    "\n",
    "#             preds_real[i] = pred_real\n",
    "\n",
    "#         # === Plotting ===\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.plot(trues_real[:, 0], trues_real[:, 2], label=\"True\", linewidth=2)\n",
    "#         plt.plot(preds_real[:, 0], preds_real[:, 2], label=\"Predicted\", linewidth=2, linestyle=\"--\")\n",
    "#         plt.title(f\"X vs Z Trajectory: {layout_name}\")\n",
    "#         plt.xlabel(\"X Coordinate\")\n",
    "#         plt.ylabel(\"Z Coordinate\")\n",
    "#         plt.axis(\"equal\")\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "#         # === Accuracy report ===\n",
    "#         print_feature_accuracy(preds_real, trues_real, scaler_y, config[\"output_cols\"])\n",
    "\n",
    "#         # === Spatial error metrics (just X/Z)\n",
    "#         spatial_errors = np.linalg.norm(preds_real[:, [0, 2]] - trues_real[:, [0, 2]], axis=1)\n",
    "#         mean_spatial_error = np.mean(spatial_errors)\n",
    "#         max_spatial_error = np.max(spatial_errors)\n",
    "#         print(f\"Mean X/Z spatial error: {mean_spatial_error:.2f}m, Max: {max_spatial_error:.2f}m\\n\")\n",
    "\n",
    "# === Run it ===\n",
    "run_inference(data_folder=\"./data/testing_layouts\", model_path=\"./models/testing_racing_proto_cerberus.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newracist",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
